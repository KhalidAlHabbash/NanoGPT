{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhalidAlHabbash/NanoGPT/blob/main/NanoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_3YPTErKKW_",
        "outputId": "81909ab7-a1e3-43e1-d4f5-b752b0587ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-09 18:56:44--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  3.35MB/s    in 0.3s    \n",
            "\n",
            "2025-11-09 18:56:45 (3.35 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 64 # independent sequences processed in parallel\n",
        "context_size = 256 # maximum context length\n",
        "max_iters = 5000\n",
        "eval_interval = 500 # every 500 intervals evaluate the model\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YYhAdzAJMA_K"
      },
      "outputs": [],
      "source": [
        "# read in input text\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7HbzljMMyw",
        "outputId": "498cfd78-6b0f-4f90-ae31-b43ee7756a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ukWxtIu7MdEo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200019\n"
          ]
        }
      ],
      "source": [
        "# Tiktoken o200k_base tokenizer\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "print(enc.n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GCTt8-3M0ez",
        "outputId": "33cb0826-af9c-4d55-a32e-1dda04dcb9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([297606])\n",
            "tensor([  7127,  84479,    734,  13036,    581,  18988,   1062,   6544,     11,\n",
            "          9598,    668,  10591,    364,   2594,    734, 116872,     11,  10591,\n",
            "           364,   7127,  84479,    734,   3575,    553,    722,  33944,   7542,\n",
            "           316,   1076,   1572,    316,   2079,   1109,   1715,   2594,    734,\n",
            "         80773,     13,  33944,    364,   7127,  84479,    734,   7127,     11,\n",
            "           481,   1761, 149492,    385,   3145, 137928,    382,  20915,  20935,\n",
            "           316,    290,   1665,    364,   2594,    734,   2167,   1761,   1507,\n",
            "            11,    581,   1761,   1507,    364,   7127,  84479,    734,  12845,\n",
            "           765,  15874,   2395,     11,    326,  22782,    679,  33994,    540,\n",
            "          1039,   2316,   3911,    558,   3031,   1507,    261,  75722,   1715,\n",
            "          2594,    734,   3160,    945,  11695,    402,   1507,     26,   1632,\n",
            "           480,    413,   4167,     25,   4194,     11,   4194,   1703,  17422,\n",
            "         84479,    734,   5045,   2195,     11,   1899,  19466,    364,   7127,\n",
            "         84479,    734,   2167,    553,  83982,  12530,  19466,     11,    290,\n",
            "          2506,   2740,  11413,   1899,    558,   4827,  20515,   1512,   2302,\n",
            "          1348,    402,   1481,  61327,    765,     25,    538,   1023,    198,\n",
            "         83527,  14376,    765,    889,    290,   2539,  32844,    536,     11,\n",
            "          2049,    480,   1504,    198,   2078,   8795,    747,     11,    581,\n",
            "          3572,  11915,   1023,  91895,    765,   5396,   1151,    307,   8293,\n",
            "          1023,   2411,    581,    553,   3101,  36203,     25,    290,    505,\n",
            "           934,    436,    484,    198,   2857, 111894,    765,     11,    290,\n",
            "          2817,    328,   1039, 117394,     11,    382,    472,    448,    198,\n",
            "         69660,    316,   5024,   1096,   1043,  50280,     26,   1039,    198,\n",
            "            82,   3299,    766,    382,    261,  11621,    316,   1373,   9024,\n",
            "           765,  72519,    495,    483,    198,    401,    275,  13396,     11,\n",
            "         16466,    581,   5025,    428,   6861,     25,    395,    290,  52901,\n",
            "          1761,    357,    198,     82,  42583,    495,    306,  53080,    395,\n",
            "         19544,     11,    625,    306,  79547,    395,  72519,    364,  17422,\n",
            "         84479,    734,  43762,    481,  18988,   6980,   4372, 149492,    385,\n",
            "          3145, 137928,   1715,   2594,    734, 141068,   2395,   1577,     25,\n",
            "         19016,    261,   1869,   6446,    316,    290,   5355,  59531,    364,\n",
            "         17422,  84479,    734,  49377,    481,   1412,   3581,    501,    853,\n",
            "          4167,    395,   1232,   4931,   1715,   7127,  84479,    734,  29061,\n",
            "          1775,     26,    326,   2023,    413,   3100,    316,   3644,   2395,\n",
            "          1899,    198,  22869,   8024,     11,    889,    484,    501,  15236,\n",
            "         11166,    483,   2447,  15164,    364,  17422,  84479,    734,     45,\n",
            "           356,     11,    889,  10591,    625,  73880,    423,    364,   7127,\n",
            "         84479,    734,     40,   2891,  47329,    481,     11,   1412,    501,\n",
            "         94658,   4167, 125114,     11,    501,   2242,    198,    278,    316,\n",
            "           484,   1268,     25,   5495,  10143,  21675,   2786,  54634,   1966,\n",
            "           665,    413,    198,   3252,    316,   2891,    480,    673,    395,\n",
            "          1232,   4931,    501,   2242,    480,    316,    198,  48576,   1232,\n",
            "         10005,    326,    316,    413,  53958,  15164,     26,   1118,    501,\n",
            "           198,    276,     11,   1952,   7892,    290,  66032,    328,   1232,\n",
            "         74782,    364,  17422,  84479,    734,   4827,    501,   6284,   1652,\n",
            "           306,   1232,   7867,     11,    481,   3527,    261,    198,   2554,\n",
            "           306,   2395,     13,   1608,   2804,    306,    860,   2006,   2891,\n",
            "           501,    382,   8885,    292,    784,    364,   7127,  84479,    734,\n",
            "          3335,    357,   2804,    625,     11,    357,   1309,    625,    413,\n",
            "        190982,    328,  97313,    307,    273,  94658,  92346,     11,    483,\n",
            "         78529,     11,    316,  30796,    306, 100556,    558,   4827,    641,\n",
            "         16513,    553,   1879,     30,    623,   1273,   4307,    293,      6,\n",
            "           290,   5030,    198,    276,  79645,     25,   4436,   5092,    581,\n",
            "           550,   1365,   2105,     30,    316,    290,  61820,   1703,   2594,\n",
            "           734,  37788,     11,   3063,    364,   7127,  84479,    734,  35689,\n",
            "             0,   1218,   5124,   2105,   1715])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize entire input text\n",
        "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "print(data.shape)\n",
        "print(data[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV0GeEiqNd5G",
        "outputId": "8a04355b-a45a-4a5d-aa13-6717d2625c65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "267845 29761\n"
          ]
        }
      ],
      "source": [
        "#Split into training and validation sets (90/10)\n",
        "train_percentage = int(0.9 * len(data))\n",
        "train_set = data[:train_percentage]\n",
        "val_set = data[train_percentage:]\n",
        "print(len(train_set), len(val_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb0tOql1OVGk",
        "outputId": "368b89e0-232d-4ef6-edd8-9e3fd458ab40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  7127,  84479,    734,  13036,    581,  18988,   1062,   6544,     11,\n",
            "          9598,    668,  10591,    364,   2594,    734, 116872,     11,  10591,\n",
            "           364,   7127,  84479,    734,   3575,    553,    722,  33944,   7542,\n",
            "           316,   1076,   1572,    316,   2079,   1109,   1715,   2594,    734,\n",
            "         80773,     13,  33944,    364,   7127,  84479,    734,   7127,     11,\n",
            "           481,   1761, 149492,    385,   3145, 137928,    382,  20915,  20935,\n",
            "           316,    290,   1665,    364,   2594,    734,   2167,   1761,   1507,\n",
            "            11,    581,   1761,   1507,    364,   7127,  84479,    734,  12845,\n",
            "           765,  15874,   2395,     11,    326,  22782,    679,  33994,    540,\n",
            "          1039,   2316,   3911,    558,   3031,   1507,    261,  75722,   1715,\n",
            "          2594,    734,   3160,    945,  11695,    402,   1507,     26,   1632,\n",
            "           480,    413,   4167,     25,   4194,     11,   4194,   1703,  17422,\n",
            "         84479,    734,   5045,   2195,     11,   1899,  19466,    364,   7127,\n",
            "         84479,    734,   2167,    553,  83982,  12530,  19466,     11,    290,\n",
            "          2506,   2740,  11413,   1899,    558,   4827,  20515,   1512,   2302,\n",
            "          1348,    402,   1481,  61327,    765,     25,    538,   1023,    198,\n",
            "         83527,  14376,    765,    889,    290,   2539,  32844,    536,     11,\n",
            "          2049,    480,   1504,    198,   2078,   8795,    747,     11,    581,\n",
            "          3572,  11915,   1023,  91895,    765,   5396,   1151,    307,   8293,\n",
            "          1023,   2411,    581,    553,   3101,  36203,     25,    290,    505,\n",
            "           934,    436,    484,    198,   2857, 111894,    765,     11,    290,\n",
            "          2817,    328,   1039, 117394,     11,    382,    472,    448,    198,\n",
            "         69660,    316,   5024,   1096,   1043,  50280,     26,   1039,    198,\n",
            "            82,   3299,    766,    382,    261,  11621,    316,   1373,   9024,\n",
            "           765,  72519,    495,    483,    198,    401,    275,  13396,     11,\n",
            "         16466,    581,   5025,    428,   6861,     25,    395,    290,  52901,\n",
            "          1761,    357,    198,     82,  42583,    495,    306,  53080,    395,\n",
            "         19544,     11,    625,    306,  79547,    395,  72519,    364,  17422,\n",
            "         84479,    734,  43762,    481])\n"
          ]
        }
      ],
      "source": [
        "print(train_set[:context_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(type):\n",
        "    data = train_set if type == \"train\" else val_set\n",
        "    # Generate random ints to access data (no. of random ints = batch_size)\n",
        "    indices = torch.randint(len(data) - context_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+context_size] for i in indices])\n",
        "    # y is off by 1 so that we can generate multiple training examples from just one input\n",
        "    y = torch.stack([data[i+1:i+context_size+1] for i in indices])\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_loss():\n",
        "    out = {}\n",
        "    model.eval() # Preserve computation, no need to calculate gradients\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in eval_iters:\n",
        "            x,y = get_batch(split)\n",
        "            logits, loss = model(x,y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class attentionHead(nn.Module):\n",
        "    \"\"\" Single head of attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # Linear layer (x * W(Q,K,V) without the bias)\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) \n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False) \n",
        "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
        "        self.dropout = nn.Dropout(dropout) # add dropout to prevent overfitting and encourage more robustness\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        k = self.key(x) # get key vector (B,T, head_size)\n",
        "        q = self.query(x) # get query vector (B,T, head_size)\n",
        "\n",
        "        # compute attention scores\n",
        "        attentionScores = q @ k.transpose(-2,-1) * k.shape[-1] ** -0.5 # --> (B,T,T)\n",
        "        attentionScores = attentionScores.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # Mask future tokens in current context\n",
        "        attentionScores = F.softmax(attentionScores, dim=-1) # apply softmax across each row (represets one token query)\n",
        "        attentionScores = self.dropout(attentionScores)\n",
        "\n",
        "        v = self.value(x) # (B,T,head_size)\n",
        "        out = attentionScores @ v # (B,T,T) @ (B,T,head_size) = (B,T,head_size)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiAttentionHead(nn.Module):\n",
        "    \"\"\" multiple heads of attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, head_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.attentionHeads = nn.ModuleList([attentionHead(head_size) for _ in range(num_heads)]) \n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        concat = torch.cat([ah(x) for ah in self.attentionHeads], dim=-1) # concatenate all feature vectors across each attention head\n",
        "        return self.dropout(self.proj(concat)) # mix information across concatenated feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    \"\"\" mlp layer of the neural network (linear layer followed by non-linear layer)\"\"\" \n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_heads\n",
        "        self.multiAttentionHeads = MultiAttentionHead(head_size, n_heads)\n",
        "        self.mlp = MultiLayerPerceptron(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.multiAttentionHeads(self.ln1(x)) # maintain residual connections\n",
        "        x = x + self.mlp(self.ln2(x)) # add new result vector of the mlp layer back in to the original vector\n",
        "        return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPRObwYrWL5F2xEr9VeEc42",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv (3.11.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
