# NanoGPT

6-layer Transformer with 6 attention heads per layer (each 64-dimensional), LayerNorm and residual connections throughout, 384-dimensional token embeddings, and a feed-forward MLP in each block.

## Note: 164 M parameter GPT. Error shown during generation due to lack of GPU, run this on a GPU to see GPT in action
