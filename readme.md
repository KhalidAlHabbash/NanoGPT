# NanoGPT

6-layer Transformer with 6 attention heads per layer (each 64-dimensional), LayerNorm and residual connections throughout, 384-dimensional token embeddings, and a feed-forward MLP in each block.

!! Run on GPU or you'll fry your laptop.
